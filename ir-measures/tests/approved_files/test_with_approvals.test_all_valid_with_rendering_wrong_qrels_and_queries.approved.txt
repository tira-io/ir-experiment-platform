ℹ Check qrels path: test-input/test-input-cranfield/qrels.txt
✓ Qrels path is valid.
ℹ Load qrels with ir-measures.
✓ Qrels successfully loaded.
ℹ Check topics path: test-input/topics_sample_valid.jsonl
✓ Topics path is valid.
ℹ Load topics.
✓ Topics successfully loaded.
ℹ Parse measures: P@2, nDCG@2
✓ Measures successfully parsed.
ℹ Check output path: test-output
✓ Output path is valid.
ℹ Export metrics.
✓ Metrics successfully exported.
ℹ Check run path: test-input/test-input-cranfield/run.txt
✓ Run path is valid.
ℹ Check run file format.
	⚠ Ranks do not start at 0.
	⚠ Scores not in descending order at lines: 11<12 
	⚠ Ranks not in ascending order at lines: 11>12 
	⚠ Ranks not consecutive at lines: 11↛12 
⚠ Run file format is valid: 4 warnings
ℹ Load run with ir-measures.
✓ Run successfully loaded.
ℹ Check run, qrels, and topics consistency.
	⚠ Document IDs of run file not found in qrels file: 359, 486, 573, 663, 746 (+1 more)
⚠ Run, qrels, and topics are inconsistent: 6 warnings
ℹ Evaluate run with measures: P@2, nDCG@2
✓ Run successfully evaluated.
ℹ Export metrics.
✓ Metrics successfully exported.


####
files: ['test-output/.data-top-10-for-rendering.json.gz', 'test-output/evaluation-per-query.prototext', 'test-output/evaluation.prototext']


####test-output/evaluation-per-query.prototext
measure {
	query_id: "1"
	measure: "P@2"
	value: "0.5"
}
measure {
	query_id: "1"
	measure: "nDCG@2"
	value: "0.4598603945740938"
}


####test-output/evaluation.prototext
measure {
	key: "P@2"
	value: "0.5"
}
measure {
	key: "intermediate_processed_queries_judged"
	value: "1"
}
measure {
	key: "nDCG@2"
	value: "0.4598603945740938"
}
